import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from google.generativeai import GenerativeModel, configure

# âœ… Cáº¥u hÃ¬nh API key Gemini tá»« secrets
configure(api_key=st.secrets["GEMINI_API_KEY"])

# âœ… Khá»Ÿi táº¡o model Gemini
model = GenerativeModel("models/gemini-2.0-flash-lite")

# âœ… Load FAISS index Ä‘Ã£ lÆ°u
@st.cache_resource
def load_vectorstore():
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return FAISS.load_local(
        "data_output/faiss_index",
        embeddings=embedding,
        allow_dangerous_deserialization=True
    )

vectorstore = load_vectorstore()

# âœ… UI Streamlit
st.title("ğŸ“š RAG Chatbot (dÃ¹ng FAISS + Gemini)")
query = st.text_input("ğŸ’¬ Nháº­p cÃ¢u há»i cá»§a báº¡n:")

if query:
    docs = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    prompt = f"""Answer the following question based on the context below:

    Context:
    {context}

    Question:
    {query}
    """

    response = model.generate_content(prompt)

    st.markdown("### ğŸ’¡ Tráº£ lá»i tá»« Gemini:")
    st.write(response.text)

    with st.expander("ğŸ“„ Ngá»¯ cáº£nh Ä‘Ã£ dÃ¹ng"):
        st.write(context)
