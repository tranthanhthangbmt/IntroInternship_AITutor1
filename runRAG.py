import os
os.environ["STREAMLIT_WATCHDOG_MODE"] = "poll"

import streamlit as st
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from google.generativeai import GenerativeModel, configure
import json
import pickle

# ğŸ” Cáº¥u hÃ¬nh API key tá»« Streamlit secrets
configure(api_key=st.secrets["GEMINI_API_KEY"])

# âš™ï¸ Khá»Ÿi táº¡o LLM Gemini
llm = GenerativeModel("models/gemini-2.0-flash-lite")

# ğŸ§  Load FAISS index tá»« thÆ° má»¥c Ä‘Ã£ lÆ°u
with open("data_output/faiss_index/index.pkl", "rb") as f:
    vectorstore = pickle.load(f)

# ğŸ“„ Load vÄƒn báº£n gá»‘c (náº¿u muá»‘n hiá»ƒn thá»‹)
with open("data_output/source_documents.json", "r", encoding="utf-8") as f:
    raw_docs = json.load(f)

# ğŸ’¬ Giao diá»‡n Streamlit
st.title("ğŸ“š RAG Chatbot (Tá»« dá»¯ liá»‡u FAISS Ä‘Ã£ táº¡o trÆ°á»›c)")
query = st.text_input("ğŸ’¬ Nháº­p cÃ¢u há»i cá»§a báº¡n:")

if query:
    # TÃ¬m vÄƒn báº£n liÃªn quan
    docs = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # Gá»­i prompt Ä‘áº¿n Gemini
    prompt = f"Answer based on context:\n{context}\n\nQuestion:\n{query}"
    response = llm.generate_content(prompt)

    # Hiá»ƒn thá»‹ káº¿t quáº£
    st.markdown("### ğŸ§  Tráº£ lá»i:")
    st.write(response.text)

    # Tuá»³ chá»n: hiá»ƒn thá»‹ láº¡i context
    with st.expander("ğŸ“„ Ngá»¯ cáº£nh Ä‘Ã£ dÃ¹ng"):
        st.write(context)

