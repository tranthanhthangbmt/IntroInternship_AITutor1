import streamlit as st
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from google.generativeai import GenerativeModel, configure
import json

# ğŸ” Cáº¥u hÃ¬nh API key tá»« Streamlit secrets
configure(api_key=st.secrets["GEMINI_API_KEY"])

# âš™ï¸ Khá»Ÿi táº¡o LLM Gemini
llm = GenerativeModel("models/gemini-2.0-flash-lite")

# ğŸ§  Load FAISS index tá»« thÆ° má»¥c Ä‘Ã£ lÆ°u
#embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#vectorstore = FAISS.load_local("data_output/faiss_index", embeddings=embedding)
from langchain.vectorstores import FAISS
import pickle
import json

with open("data_output/faiss_index/index.pkl", "rb") as f:
    vectorstore = pickle.load(f)

# Sau Ä‘Ã³ dÃ¹ng vectorstore nhÆ° bÃ¬nh thÆ°á»ng...


# ğŸ“„ Load vÄƒn báº£n gá»‘c (náº¿u muá»‘n hiá»ƒn thá»‹)
with open("data_output/source_documents.json", "r", encoding="utf-8") as f:
    raw_docs = json.load(f)

# ğŸ’¬ Giao diá»‡n Streamlit
st.title("ğŸ“š RAG Chatbot (Tá»« dá»¯ liá»‡u FAISS Ä‘Ã£ táº¡o trÆ°á»›c)")
query = st.text_input("ğŸ’¬ Nháº­p cÃ¢u há»i cá»§a báº¡n:")

if query:
    # TÃ¬m vÄƒn báº£n liÃªn quan
    docs = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # Gá»­i prompt Ä‘áº¿n Gemini
    prompt = f"Answer based on context:\n{context}\n\nQuestion:\n{query}"
    response = llm.generate_content(prompt)

    # Hiá»ƒn thá»‹ káº¿t quáº£
    st.markdown("### ğŸ§  Tráº£ lá»i:")
    st.write(response.text)

    # Tuá»³ chá»n: hiá»ƒn thá»‹ láº¡i context
    with st.expander("ğŸ“„ Ngá»¯ cáº£nh Ä‘Ã£ dÃ¹ng"):
        st.write(context)
