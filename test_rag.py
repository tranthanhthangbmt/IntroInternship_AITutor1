# ‚úÖ M·ªõi:
from langchain_community.vectorstores import FAISS
#!from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings

from google.generativeai import GenerativeModel, configure

# C·∫•u h√¨nh API key Gemini
configure(api_key="AIzaSyB23c7ttZ-RWiqj9O4dY82NutHsjz0N45s")  # ‚úÖ Thay b·∫±ng API key th·ª±c

# Kh·ªüi t·∫°o model Gemini
model = GenerativeModel("models/gemini-2.0-flash-lite")

# Load FAISS index ƒë√£ l∆∞u
#!embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L3-v2")
vectorstore = FAISS.load_local(
    "data_output/faiss_index",
    embeddings=embedding,
    allow_dangerous_deserialization=True
)

# V√≤ng l·∫∑p ƒë·ªÉ ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi
print("ü§ñ Mini RAG Chatbot - nh·∫≠p 'exit' ƒë·ªÉ tho√°t")
while True:
    query = input("‚ùì B·∫°n h·ªèi g√¨: ").strip()
    if query.lower() == "exit":
        break

    # Truy xu·∫•t context li√™n quan
    docs = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # G·ª≠i ƒë·∫øn Gemini
    prompt = f"""Answer the following question based on the context below:

    Context:
    {context}

    Question:
    {query}
    """
    response = model.generate_content(prompt)
    print("\nüí° Tr·∫£ l·ªùi t·ª´ Gemini:\n", response.text)
    print("=" * 50)
