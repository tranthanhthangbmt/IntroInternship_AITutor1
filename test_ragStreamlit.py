import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from google.generativeai import GenerativeModel, configure

# ‚ö†Ô∏è C·∫•u h√¨nh API key Gemini (thay b·∫±ng key th·ª±c t·∫ø ho·∫∑c d√πng dotenv)
configure(api_key="AIzaSyB23c7ttZ-RWiqj9O4dY82NutHsjz0N45s")

# Kh·ªüi t·∫°o model Gemini
model = GenerativeModel("models/gemini-2.0-flash-lite")

# Load FAISS index
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L3-v2")
vectorstore = FAISS.load_local(
    "data_output/faiss_index",
    embeddings=embedding,
    allow_dangerous_deserialization=True
)

# C·∫•u h√¨nh giao di·ªán Streamlit
st.set_page_config(page_title="Mini RAG Chatbot", page_icon="ü§ñ")
st.title("ü§ñ Mini RAG Chatbot")
st.caption("T√¨m ki·∫øm ng·ªØ c·∫£nh b·∫±ng FAISS & tr·∫£ l·ªùi v·ªõi Gemini 2.0")

# Kh·ªüi t·∫°o session state ƒë·ªÉ l∆∞u l·ªãch s·ª≠ chat
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Nh·∫≠p c√¢u h·ªèi
query = st.text_input("‚ùì Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")

if query:
    # Truy xu·∫•t ng·ªØ c·∫£nh li√™n quan
    docs = vectorstore.similarity_search(query, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    # T·∫°o prompt cho Gemini
    prompt = f"""Answer the following question based on the context below:

    Context:
    {context}

    Question:
    {query}
    """
    # G·ª≠i prompt ƒë·∫øn Gemini
    response = model.generate_content(prompt)
    answer = response.text.strip()

    # L∆∞u l·ªãch s·ª≠ Q&A
    st.session_state.chat_history.append({
        "question": query,
        "answer": answer
    })

# Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i (m·ªõi nh·∫•t ·ªü d∆∞·ªõi c√πng)
for chat in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(chat["question"])
    with st.chat_message("ai"):
        st.markdown(chat["answer"])
