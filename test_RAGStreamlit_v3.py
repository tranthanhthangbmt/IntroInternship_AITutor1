#run: streamlit run test_RAGStreamlit.py
import os
os.environ["STREAMLIT_WATCH_FILE_SYSTEM"] = "false"
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from google.generativeai import GenerativeModel, configure

# âš ï¸ Cáº¥u hÃ¬nh API key Gemini (thay báº±ng key thá»±c táº¿ hoáº·c dÃ¹ng dotenv)
configure(api_key="AIzaSyB23c7ttZ-RWiqj9O4dY82NutHsjz0N45s")

# Khá»Ÿi táº¡o model Gemini
model = GenerativeModel("models/gemini-2.0-flash-lite")

# Load FAISS index
#embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
#embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L3-v2")
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")


vectorstore = FAISS.load_local(
    "IntroInternshipRAG/faiss_index",
    embeddings=embedding,
    allow_dangerous_deserialization=True
)

# Cáº¥u hÃ¬nh giao diá»‡n Streamlit
st.set_page_config(page_title="Tutor AI â€“ Há»— trá»£ Thá»±c táº­p CNTT", page_icon="ğŸ“")
# Sidebar â€“ hiá»ƒn thá»‹ logo vÃ  thÃ´ng tin
with st.sidebar:
    st.image("https://raw.githubusercontent.com/tranthanhthangbmt/AITutor_Gemini/main/LOGO_UDA_2023_VN_EN_chuan2.png", width=180)
    st.markdown("""
    ### ğŸ“ Tutor AI â€“ Äáº¡i há»c ÄÃ´ng Ã
    **Há»— trá»£ sinh viÃªn thá»±c táº­p ngÃ nh CNTT**

    ---
    ğŸ“ *Má»i tháº¯c máº¯c vui lÃ²ng nháº­p bÃªn dÆ°á»›i Ä‘á»ƒ Ä‘Æ°á»£c giáº£i Ä‘Ã¡p.*
    """)
    
st.title("ğŸ“ Tutor AI - Há»— trá»£ Thá»±c táº­p CNTT")
#st.caption("TÃ¬m kiáº¿m ngá»¯ cáº£nh báº±ng FAISS & tráº£ lá»i vá»›i Gemini 2.0")
with st.chat_message("assistant"):
    st.markdown("""
    ğŸ‘‹ **Xin chÃ o!**  
    TÃ´i lÃ  **Tutor AI** â€“ trá»£ lÃ½ áº£o há»— trá»£ sinh viÃªn thá»±c hiá»‡n **Thá»±c táº­p Nháº­n Thá»©c ngÃ nh CÃ´ng nghá»‡ ThÃ´ng tin** táº¡i TrÆ°á»ng Äáº¡i há»c ÄÃ´ng Ã.

    ğŸ¯ Nhiá»‡m vá»¥ cá»§a tÃ´i:
    - HÆ°á»›ng dáº«n báº¡n náº¯m rÃµ ná»™i dung, yÃªu cáº§u vÃ  lá»‹ch trÃ¬nh thá»±c táº­p
    - TÆ° váº¥n cÃ¡ch ghi nháº­t kÃ½, viáº¿t bÃ¡o cÃ¡o Ä‘Ãºng chuáº©n
    - Tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n: **máº«u biá»ƒu**, **Ä‘Ã¡nh giÃ¡**, **bÃ¡o cÃ¡o tuáº§n**, **thÃ¡i Ä‘á»™ - ká»¹ nÄƒng nghá» nghiá»‡p**
    - Giáº£i thÃ­ch quy trÃ¬nh thá»±c táº­p vÃ  giÃºp báº¡n Ä‘á»‹nh hÆ°á»›ng nghá» nghiá»‡p

    âœï¸ HÃ£y nháº­p cÃ¢u há»i bÃªn dÆ°á»›i nhÆ°:
    - â€œCáº§n ná»™p nhá»¯ng biá»ƒu máº«u nÃ o trong thá»±c táº­p?â€
    - â€œMáº«u nháº­t kÃ½ thá»±c táº­p viáº¿t tháº¿ nÃ o?â€
    - â€œBÃ i toÃ¡n thá»±c táº­p lÃ  gÃ¬? LÃ m sao Ä‘á»ƒ chá»n?â€
    
    TÃ´i luÃ´n sáºµn sÃ ng Ä‘á»“ng hÃ nh cÃ¹ng báº¡n trong suá»‘t 10 tuáº§n thá»±c táº­p ğŸ¤
    """)

# Khá»Ÿi táº¡o session state Ä‘á»ƒ lÆ°u lá»‹ch sá»­ chat
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Nháº­p cÃ¢u há»i
#query = st.text_input("â“ Nháº­p cÃ¢u há»i cá»§a báº¡n:")
query = st.chat_input("â“ Nháº­p cÃ¢u há»i cá»§a báº¡n:")


if query:
    # Truy xuáº¥t ngá»¯ cáº£nh liÃªn quan
    docs = vectorstore.similarity_search(query, k=6)
    context = "\n".join([doc.page_content for doc in docs])

    # Táº¡o prompt cho Gemini
    prompt = f"""
    Báº¡n lÃ  trá»£ lÃ½ AI Ä‘ang há»— trá»£ sinh viÃªn thá»±c táº­p CNTT.
    HÃ£y tráº£ lá»i cÃ¢u há»i dÆ°á»›i Ä‘Ã¢y má»™t cÃ¡ch chi tiáº¿t, chÃ­nh xÃ¡c vÃ  dá»… hiá»ƒu nháº¥t, dá»±a trÃªn cÃ¡c thÃ´ng tin cÃ³ trong tÃ i liá»‡u:

    Ngá»¯ cáº£nh:
    {context}

    CÃ¢u há»i:
    {query}
    """

    # Gá»­i prompt Ä‘áº¿n Gemini
    response = model.generate_content(prompt)
    answer = response.text.strip()

    # LÆ°u lá»‹ch sá»­ Q&A
    st.session_state.chat_history.append({
        "question": query,
        "answer": answer
    })

# Hiá»ƒn thá»‹ lá»‹ch sá»­ há»™i thoáº¡i (má»›i nháº¥t á»Ÿ dÆ°á»›i cÃ¹ng)
for chat in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(chat["question"])
    with st.chat_message("ai"):
        st.markdown(chat["answer"])
