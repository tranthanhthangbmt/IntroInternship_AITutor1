import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
import os
from langchain.embeddings import HuggingFaceEmbeddings

# L·∫•y API key t·ª´ Streamlit Secrets v√† √©p bu·ªôc c·∫•u h√¨nh m√¥i tr∆∞·ªùng
os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]

st.title("ü§ñ Gemini-powered RAG Chatbot")

# Load vƒÉn b·∫£n v√† t√°ch nh·ªè
loader = TextLoader("example.txt")
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)
chunks = [c for c in chunks if len(c.page_content) < 1000]

# T·∫°o danh s√°ch vƒÉn b·∫£n
texts = [doc.page_content for doc in chunks]

# Embedding v·ªõi ki·ªÉm tra to√†n b·ªô
#embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
#embedding = HuggingFaceEmbeddings()
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

try:
    #vectors = embedding.embed_documents(texts)
    #vectordb = FAISS.from_texts(texts, embedding)    
    vectordb = FAISS.from_texts(texts, embedding)
except Exception as e:
    st.error(f"‚ùå L·ªói khi t·∫°o vector DB (Gemini Embedding): {e}")
    vectors = []
    vectordb = None

# T·∫°o LLM
llm = ChatGoogleGenerativeAI(model="gemini-pro")

# Giao di·ªán nh·∫≠p v√† tr·∫£ l·ªùi
query = st.text_input("Nh·∫≠p c√¢u h·ªèi:")
if query and vectordb:
    docs = vectordb.similarity_search(query)
    context = "\n\n".join([d.page_content for d in docs])
    prompt = f"D·ª±a tr√™n vƒÉn b·∫£n sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi:\n\n{context}\n\nC√¢u h·ªèi: {query}"
    answer = llm.invoke(prompt)
    st.markdown(f"**üìå Tr·∫£ l·ªùi:** {answer.content}")
elif query:
    st.warning("‚ö† Kh√¥ng th·ªÉ truy v·∫•n v√¨ vector DB ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.")
