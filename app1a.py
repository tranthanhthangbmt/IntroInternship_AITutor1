import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
import os

st.title("ðŸ¤– RAG Chatbot - HuggingFace + Gemini LLM")

# Load dá»¯ liá»‡u
loader = TextLoader("example.txt")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(docs)
chunks = [c for c in chunks if len(c.page_content) < 1000]
texts = [doc.page_content for doc in chunks]

# Embedding báº±ng HuggingFace
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
embedding = HuggingFaceEmbeddings()
vectordb = None

try:
    vectordb = FAISS.from_texts(texts, embedding)
except Exception as e:
    st.error(f"âŒ Lá»—i khi táº¡o FAISS vector DB: {e}")

# LLM Gemini chá»‰ Ä‘á»ƒ tráº£ lá»i
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
llm = ChatGoogleGenerativeAI(model="gemini-pro")

query = st.text_input("Nháº­p cÃ¢u há»i:")
if query and vectordb:
    docs = vectordb.similarity_search(query)
    context = "\n\n".join([d.page_content for d in docs])
    prompt = f"Dá»±a trÃªn vÄƒn báº£n sau, hÃ£y tráº£ lá»i cÃ¢u há»i:\n\n{context}\n\nCÃ¢u há»i: {query}"
    answer = llm.invoke(prompt)
    st.markdown(f"**ðŸ“Œ Tráº£ lá»i:** {answer.content}")
elif query:
    st.warning("âš  KhÃ´ng thá»ƒ truy váº¥n vÃ¬ vector DB chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o.")
